{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi29kv3J9c0IEfFz7qrDtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h40300965/deep-learnin/blob/main/nfnet-%20tensofrflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK-rRLZzOyPE",
        "outputId": "f0781dd5-85f9-4138-a612-366c10d79fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.11/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQVf7Ib0RxMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“ 1. Install Required Packages"
      ],
      "metadata": {
        "id": "aLvBG4Vwzj4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sam-tf --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9wPgth_Qxqh",
        "outputId": "96c2d624-8f0d-486d-adc2-75dbfabb031f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sam-tf (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sam-tf\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§  2. Import Libraries"
      ],
      "metadata": {
        "id": "WuptwTlgzq-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, models, mixed_precision\n",
        "\n",
        "# Enable Mixed Precision (optional)\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "MA2tcchASPYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”§ 3. Scaled Weight Standardization Layer"
      ],
      "metadata": {
        "id": "7aCyfr1Kz8aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledWSConv2D(layers.Conv2D):\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        # Compute fan-in\n",
        "        kernel_shape = self.kernel.shape.as_list()\n",
        "        self.fan_in = np.prod(kernel_shape[:-1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, var = tf.nn.moments(self.kernel, axes=[0, 1, 2], keepdims=True)\n",
        "        weight = (self.kernel - mean) * tf.math.rsqrt(var + 1e-10)\n",
        "        scale = tf.math.sqrt(2. / self.fan_in)\n",
        "        x = tf.nn.conv2d(\n",
        "            inputs,\n",
        "            filters=weight * scale,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding.upper(),\n",
        "            data_format=\"NHWC\"\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            x = tf.nn.bias_add(x, self.bias)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SLSQ21SRSRTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš™ï¸ 4. NFBlock with SkipInit"
      ],
      "metadata": {
        "id": "-qYrugiy0Ckj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NFBlock(layers.Layer):\n",
        "    def __init__(self, channels, expansion=2, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        mid_channels = channels // expansion\n",
        "        self.conv1 = ScaledWSConv2D(mid_channels, 1, strides=stride, use_bias=False)\n",
        "        self.act1 = layers.Activation('gelu')\n",
        "\n",
        "        self.conv2 = ScaledWSConv2D(mid_channels, 3, strides=1, padding='same', use_bias=False)\n",
        "        self.act2 = layers.Activation('gelu')\n",
        "\n",
        "        self.conv3 = ScaledWSConv2D(channels, 1, strides=1, use_bias=False)\n",
        "        self.act3 = layers.Activation('gelu')\n",
        "\n",
        "        if stride != 1 or inputs.shape[-1] != channels:\n",
        "            self.shortcut = ScaledWSConv2D(channels, 1, strides=stride, use_bias=False)\n",
        "        else:\n",
        "            self.shortcut = tf.identity\n",
        "\n",
        "        self.skip_gain = tf.Variable(0., trainable=True, name=\"skip_gain\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = x * self.skip_gain\n",
        "\n",
        "        shortcut = self.shortcut(inputs)\n",
        "        x = x + shortcut\n",
        "        x = self.act3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eJfRLBcQSXDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§± 5. Stem Network"
      ],
      "metadata": {
        "id": "kpDJ_rOI0Jz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stem(filters):\n",
        "    return models.Sequential([\n",
        "        layers.Input((32, 32, 3)),\n",
        "        ScaledWSConv2D(filters, 3, strides=1, padding='same'),\n",
        "        layers.Activation('gelu'),\n",
        "        ScaledWSConv2D(filters, 3, strides=1, padding='same'),\n",
        "        layers.Activation('gelu'),\n",
        "        ScaledWSConv2D(filters, 3, strides=1, padding='same'),\n",
        "        layers.Activation('gelu'),\n",
        "        layers.MaxPool2D(3, strides=2, padding='same')\n",
        "    ], name=\"stem\")"
      ],
      "metadata": {
        "id": "BC3TheH4SdFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§¬ 6. Build Full NFNet Model"
      ],
      "metadata": {
        "id": "awuyxS6m0La9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_nfnet(num_classes=10):\n",
        "    stem = get_stem(64)\n",
        "    blocks = [\n",
        "        NFBlock(256),\n",
        "        NFBlock(256),\n",
        "        NFBlock(256),\n",
        "        NFBlock(512, stride=2),\n",
        "        NFBlock(512),\n",
        "        NFBlock(512),\n",
        "        NFBlock(1024, stride=2),\n",
        "        NFBlock(1024),\n",
        "        NFBlock(1024)\n",
        "    ]\n",
        "\n",
        "    head = models.Sequential([\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(num_classes)\n",
        "    ])\n",
        "\n",
        "    model = models.Sequential([\n",
        "        stem,\n",
        "        *blocks,\n",
        "        head\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "JdrxGZ2aSfPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”„ 7. Data Augmentation & CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "s8tEMAyR0QoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize\n",
        "train_images = train_images.astype(\"float32\") / 255.0\n",
        "test_images = test_images.astype(\"float32\") / 255.0\n",
        "\n",
        "# Augmentation\n",
        "augmenter = models.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.2)\n",
        "])\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 512\n",
        "num_epochs = 20\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_dataset = train_dataset.map(lambda x, y: (augmenter(x), y)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed9_eh96SjZ2",
        "outputId": "79fc2841-bb02-4ce8-b4e3-ace88d29d925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUUaMTx2VGbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš™ï¸ 8. SAM Optimizer + Loss Function\n",
        "\n",
        "  from sam_tf import SAMModel\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "model = build_nfnet(num_classes=10)\n",
        "\n",
        "# Wrap with SAM\n",
        "sam_model = SAM(model, distance=0.05)\n",
        "\n",
        "# Compile\n",
        "sam_model.compile(\n",
        "    optimizer=keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-4),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "X7BvzuklzNJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Manually Implement SAM"
      ],
      "metadata": {
        "id": "gAu2tan-1C2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WS2kLKOH04ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class SAMModel(keras.Model):\n",
        "    def __init__(self, base_model, rho=0.05):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.rho = rho  # Perturbation radius (called \"distance\" in your code)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        (images, labels) = data\n",
        "\n",
        "        # First forward-backward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self.base_model(images, training=True)\n",
        "            loss = self.compiled_loss(labels, logits)\n",
        "        gradients = tape.gradient(loss, self.base_model.trainable_variables)\n",
        "\n",
        "        # Apply SAM perturbation\n",
        "        grad_norm = tf.linalg.global_norm(gradients)\n",
        "        scale = self.rho / (grad_norm + 1e-12)\n",
        "        perturbations = [g * scale for g in gradients]\n",
        "\n",
        "        # Save original weights and apply perturbations\n",
        "        original_weights = [tf.identity(w) for w in self.base_model.weights]\n",
        "        for w, p in zip(self.base_model.trainable_variables, perturbations):\n",
        "            w.assign_add(p)\n",
        "\n",
        "        # Second forward-backward pass\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self.base_model(images, training=True)\n",
        "            loss = self.compiled_loss(labels, logits)\n",
        "        gradients = tape.gradient(loss, self.base_model.trainable_variables)\n",
        "\n",
        "        # Restore original weights\n",
        "        for w, orig in zip(self.base_model.weights, original_weights):\n",
        "            w.assign(orig)\n",
        "\n",
        "        # Update weights with the gradients from the second pass\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(gradients, self.base_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(labels, logits)\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "Tls-2HJWUzLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update Your Code   Manually Implement SAM\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MNDTBAz8zSrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Replace this with your actual NFNet builder\n",
        "def build_nfnet(input_shape=(224, 224, 3), num_classes=10):\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Input(shape=input_shape),\n",
        "        keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "        keras.layers.GlobalAveragePooling2D(),\n",
        "        keras.layers.Dense(num_classes)\n",
        "    ])\n",
        "\n",
        "# Build model and wrap with SAM\n",
        "base_model = build_nfnet(num_classes=10)\n",
        "sam_model = SAMModel(base_model, rho=0.05)  # Use rho=0.05 instead of distance\n",
        "\n",
        "# Compile and train\n",
        "sam_model.compile(\n",
        "    optimizer=keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-4),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# sam_model.fit(x_train, y_train, epochs=10, ...)"
      ],
      "metadata": {
        "id": "qmt3ZQ-_U3Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8nM-EVsoFFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‹ï¸â€â™‚ï¸ 9. Train the Model"
      ],
      "metadata": {
        "id": "Pm-SKW6T2Jlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = sam_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=num_epochs\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe829f2-0468-4d3d-a171-f86b29e0dbe9",
        "id": "fiAJNX_bVIlI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:667: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight, training)`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:642: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
            "```\n",
            "for metric in self.metrics:\n",
            "    metric.update_state(y, y_pred)\n",
            "```\n",
            "\n",
            "  return self._compiled_metrics_update_state(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 933ms/step - accuracy: 0.1014 - loss: -0.0081 - val_accuracy: 0.1160 - val_loss: 2.2678\n",
            "Epoch 2/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 956ms/step - accuracy: 0.1286 - loss: -0.0084 - val_accuracy: 0.1537 - val_loss: 2.2374\n",
            "Epoch 3/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 903ms/step - accuracy: 0.1547 - loss: -2.9738e-04 - val_accuracy: 0.1782 - val_loss: 2.2049\n",
            "Epoch 4/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 895ms/step - accuracy: 0.1675 - loss: 4.2788e-04 - val_accuracy: 0.1933 - val_loss: 2.1742\n",
            "Epoch 5/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 944ms/step - accuracy: 0.1764 - loss: -0.0025 - val_accuracy: 0.2034 - val_loss: 2.1495\n",
            "Epoch 6/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 987ms/step - accuracy: 0.1840 - loss: -0.0109 - val_accuracy: 0.2064 - val_loss: 2.1305\n",
            "Epoch 7/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 892ms/step - accuracy: 0.1875 - loss: -0.0168 - val_accuracy: 0.2247 - val_loss: 2.1167\n",
            "Epoch 8/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 885ms/step - accuracy: 0.1920 - loss: -0.0263 - val_accuracy: 0.2288 - val_loss: 2.1055\n",
            "Epoch 9/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 885ms/step - accuracy: 0.1987 - loss: -0.0340 - val_accuracy: 0.2427 - val_loss: 2.0973\n",
            "Epoch 10/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 914ms/step - accuracy: 0.1982 - loss: -0.0355 - val_accuracy: 0.2293 - val_loss: 2.0897\n",
            "Epoch 11/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 908ms/step - accuracy: 0.2038 - loss: -0.0485 - val_accuracy: 0.2378 - val_loss: 2.0835\n",
            "Epoch 12/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 980ms/step - accuracy: 0.2042 - loss: -0.0601 - val_accuracy: 0.2474 - val_loss: 2.0783\n",
            "Epoch 13/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 881ms/step - accuracy: 0.2107 - loss: -0.0662 - val_accuracy: 0.2437 - val_loss: 2.0734\n",
            "Epoch 14/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 885ms/step - accuracy: 0.2137 - loss: -0.0772 - val_accuracy: 0.2499 - val_loss: 2.0691\n",
            "Epoch 15/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 881ms/step - accuracy: 0.2129 - loss: -0.0811 - val_accuracy: 0.2423 - val_loss: 2.0647\n",
            "Epoch 16/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 882ms/step - accuracy: 0.2132 - loss: -0.0878 - val_accuracy: 0.2529 - val_loss: 2.0615\n",
            "Epoch 17/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 889ms/step - accuracy: 0.2187 - loss: -0.0970 - val_accuracy: 0.2519 - val_loss: 2.0573\n",
            "Epoch 18/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - accuracy: 0.2191 - loss: -0.1043 - val_accuracy: 0.2395 - val_loss: 2.0542\n",
            "Epoch 19/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 889ms/step - accuracy: 0.2189 - loss: -0.1026 - val_accuracy: 0.2518 - val_loss: 2.0501\n",
            "Epoch 20/20\n",
            "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 889ms/step - accuracy: 0.2213 - loss: -0.1168 - val_accuracy: 0.2592 - val_loss: 2.0469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“Š 10. Evaluate and Save"
      ],
      "metadata": {
        "id": "x5Iqf6yo2MY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = sam_model.evaluate(test_dataset)\n",
        "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWZQfzgIxQzV",
        "outputId": "e5a6d1a6-acbb-457b-c77b-8102f9d77434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.2646 - loss: 2.0527\n",
            "Test Accuracy: 25.92%\n"
          ]
        }
      ]
    }
  ]
}